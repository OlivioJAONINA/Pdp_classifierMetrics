
# ğŸš€ ML Pipeline avec Airflow, MLflow & Streamlit

Ce projet propose un pipeline complet de Machine Learning orchestrÃ© par **Apache Airflow**, avec **prÃ©traitement**, **entraÃ®nement automatique**, **tracking des modÃ¨les avec MLflow** et **visualisation via Streamlit**.

---

## ğŸ§° Technologies utilisÃ©es

- Python 3.9+
- Apache Airflow
- MLflow
- Scikit-learn
- pandas, numpy, joblib, combat
- Docker & Docker Compose
- Streamlit
- PostgreSQL (Airflow backend)
---

## ğŸ“¦ Structure du projet

```
.
â”œâ”€â”€ dags/                  # DAG Airflow (ml_pipeline.py)
â”œâ”€â”€ data/                 
â”‚   â”œâ”€â”€ raw/               # Fichiers CSV bruts
â”‚   â””â”€â”€ processed/         # DonnÃ©es transformÃ©es
â”œâ”€â”€ mlruns/                # Artifacts MLflow
â”œâ”€â”€ models/                # ModÃ¨les ML sauvegardÃ©s
â”œâ”€â”€ scripts/               
â”‚   â”œâ”€â”€ processing.py
â”‚   â”œâ”€â”€ train_model.py
â”‚   â”œâ”€â”€ check_new_data.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â””â”€â”€ streamlit_app.py   # Interface utilisateur
â”œâ”€â”€ Dockerfile.mlflow      # Image MLflow
â”œâ”€â”€ docker-compose.yml     # Lancement global
â””â”€â”€ README.md
```

---

## ğŸ’» PrÃ©-requis

### 1. Installer Docker + Docker Compose

#### ğŸ§ Linux

```bash
sudo apt update
sudo apt install docker.io docker-compose -y
sudo usermod -aG docker $USER
newgrp docker
```

#### ğŸ macOS

- Installez Docker Desktop depuis [https://www.docker.com/products/docker-desktop/](https://www.docker.com/products/docker-desktop/)
- Activez l'extension **"Containers"** si vous utilisez **VSCode**

#### ğŸªŸ Windows

- Installez **Docker Desktop for Windows**
- Activez :
  - **WSL2 Backend**
  - **Integration VSCode (Containers Extension)**

ğŸ‘‰ RedÃ©marrez votre machine aprÃ¨s installation.

---

## âš™ï¸ Lancement du projet

1. Clonez ce dÃ©pÃ´t :

```bash
git clone <lien_du_repo>
cd <nom_du_dossier>
```

2. Lancez tous les services :

```bash
docker-compose up --build
```

Cela dÃ©marre :
- le serveur Airflow (`localhost:8080`)
- le serveur MLflow (`localhost:5000`)
- l'interface Streamlit (`localhost:8501`)

---

## ğŸŒ€ AccÃ¨s aux interfaces

| Interface       | URL                     | Identifiants |
|----------------|--------------------------|--------------|
| Airflow        | http://localhost:8080    | admin/admin  |
| MLflow         | http://localhost:5000    | (pas dâ€™auth) |
| Streamlit App  | http://localhost:8501    |              |

---

## ğŸ“Š Utilisation

1. DÃ©posez un fichier `.csv` dans `data/raw/`
2. Le DAG Airflow `ml_pipeline` (planifiÃ© quotidiennement) :
   - vÃ©rifie les nouveaux fichiers
   - prÃ©traite les donnÃ©es (corrÃ©lation, PCA, ComBat)
   - entraÃ®ne plusieurs modÃ¨les pour deux cibles (`Class_simple` et `sick`)
   - sauvegarde le meilleur modÃ¨le dans `models/`
   - loggue les expÃ©riences sur MLflow

3. Lancez lâ€™interface Streamlit (`localhost:8501`)
   - uploadez un nouveau fichier CSV
   - les donnÃ©es sont transformÃ©es selon le pipeline
   - les prÃ©dictions sont affichÃ©es

---

## ğŸ“ Configuration des modÃ¨les

Dans `scripts/config.yaml`, vous pouvez personnaliser :

```yaml
models:
  LogisticRegression:
    estimator: sklearn.linear_model.LogisticRegression
    param_grid:
      max_iter: [500, 1000]
      C: [0.1, 1, 10]
```

Ajoutez d'autres modÃ¨les et grilles dâ€™hyperparamÃ¨tres ici.

---

## ğŸ§ª Test rapide (optionnel)

```bash
docker-compose exec webserver airflow dags trigger -d "Trigger manuel" ml_pipeline
```

---

## ğŸ§¹ Nettoyage

```bash
docker-compose down -v
```

---

## âœ¨ Ã€ venir

- Monitoring Prometheus/Grafana
- Sauvegarde vers MinIO / S3
- DÃ©ploiement automatique du modÃ¨le (API FastAPI)

---

## ğŸ¤ Auteurs

Projet conÃ§u par Aymen Hassin

---
